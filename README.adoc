= Flink-Trino-Superset Data Pipeline
:toc:
:icons: font
:source-highlighter: highlight.js
:diagram-server-url: https://kroki.io
:diagram-server-type: kroki_io

This project demonstrates an end-to-end data pipeline using Apache Kafka, Apache Flink, Apache Iceberg, Trino, and Apache Superset.
The pipeline ingests streaming data, processes it in real-time, stores it in a data lake format, and provides SQL querying and visualization capabilities.

== ğŸ—ï¸ Architecture

[source]
----
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â”‚  Data       â”‚ -> â”‚  Apache     â”‚ -> â”‚  Apache     â”‚ -> â”‚  Apache     â”‚ -> â”‚  Trino      â”‚ -> â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generator  â”‚    â”‚  Kafka      â”‚    â”‚  Flink      â”‚    â”‚  Iceberg    â”‚    â”‚  Query      â”‚    â”‚             â”‚
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚  Engine     â”‚    â”‚  Apache     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  Superset   â”‚
                                                                                               â”‚             â”‚
                                                                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
----

=== ğŸ“Š Data Flow Diagram

[mermaid]
....
flowchart LR
    A[Data Generator] -->|Produces Events| B[Kafka]
    B -->|Consumes Events| C[Flink]
    C -->|Processes Data| D[Iceberg]
    D -->|Stores Tables| E[Trino]
    E -->|Queries Data| F[Superset]
    F -->|Visualizes| G[Dashboards]

    style A fill:#f9d5e5,stroke:#333,stroke-width:2px
    style B fill:#eeeeee,stroke:#333,stroke-width:2px
    style C fill:#d5e8f9,stroke:#333,stroke-width:2px
    style D fill:#e5f9d5,stroke:#333,stroke-width:2px
    style E fill:#f9e5d5,stroke:#333,stroke-width:2px
    style F fill:#d5f9e5,stroke:#333,stroke-width:2px
    style G fill:#e5d5f9,stroke:#333,stroke-width:2px
....

== ğŸ§© Components

* ğŸš€ *Apache Kafka*: Message broker for data ingestion
* ğŸŒŠ *Apache Flink*: Stream processing framework
* â„ï¸ *Apache Iceberg*: Table format for data lake storage
* ğŸ” *Trino*: Distributed SQL query engine
* ğŸ“Š *Apache Superset*: Data visualization and dashboarding

== ğŸ“‹ Prerequisites

* Docker and Docker Compose (preferably https://orbstack.dev/[OrbStack] for macOS)
* Java 21 (for building Flink jobs)
* https://sdkman.io/[SDKMAN!] for managing SDKs
* Gradle (for building Flink jobs)

== ğŸš€ Getting Started

=== 1. Clone the Repository

[source,bash]
----
git clone https://github.com/yourusername/flink-trino-superset-pipeline.git
cd flink-trino-superset-pipeline
----

=== 2. Start the Services

[source,bash]
----
make up
----

This will start all the required services:

* Kafka (ports 9092, 29092)
* Kafka UI (port 8080)
* Minio (ports 9000, 9001)
* Flink JobManager (port 8081)
* Flink TaskManager
* MySQL (port 3306)
* Iceberg REST Catalog (port 8181)
* Trino (port 8082)
* Superset (port 8088)
* Data Generator

=== 3. Create Iceberg Tables

[source,bash]
----
make create-tables
----

Alternatively, you can connect to the Trino CLI manually:

[source,bash]
----
docker exec -it trino-coordinator trino --server localhost:8080 --catalog iceberg
----

And run the SQL script:

[source,sql]
----
source /opt/flink/usrlib/create_tables.sql
----

=== 4. Build and Submit Flink Jobs

[source,bash]
----
make deploy-flink-jobs
----

This will:

1. Build the Flink jobs using Gradle
2. Copy the JAR files to the Flink JobManager
3. Submit the jobs to Flink

=== 5. Configure Data Generator Environment

The data generator supports both local and cloud (Confluent Cloud) Kafka environments. By default, it uses the local configuration.

To switch between environments:

1. *Using Docker Compose*: Edit the `KAFKA_ENV` environment variable in `docker-compose.yml`:

[source,yaml]
----
data-generator:
  environment:
    # Use 'local' for local development or 'cloud' for Confluent Cloud
    KAFKA_ENV: local  # Change to 'cloud' for Confluent Cloud
----

2. *Using Environment Variable*: Set the `KAFKA_ENV` environment variable when running the container:

[source,bash]
----
docker-compose run -e KAFKA_ENV=cloud data-generator
----

The configuration files are located in the `config/` directory:

* `config/local/` - Configuration for local Kafka
* `config/cloud/` - Configuration for Confluent Cloud

=== 6. Set Up Superset

[source,bash]
----
make setup-superset
----

Then access Superset at http://localhost:8088 and log in with:

* Username: admin
* Password: admin

Configure a connection to Trino:

1. Go to Data -> Databases -> + Database
2. Select "Trino" as the database type
3. Set the SQLAlchemy URI to: `trino://admin@trino-coordinator:8080/iceberg`
4. Test the connection and save

Create datasets and dashboards:

1. Go to Data -> Datasets -> + Dataset
2. Select the Trino connection and choose tables from the iceberg.warehouse schema
3. Create visualizations and dashboards based on the data

== ğŸ”— Accessing the Services

* ğŸš€ *Kafka UI*: http://localhost:8080
* ğŸ—„ï¸ *Minio Console*: http://localhost:9001 (minioadmin/minioadmin)
* ğŸŒŠ *Flink Dashboard*: http://localhost:8081
* ğŸ” *Trino UI*: http://localhost:8082
* ğŸ“Š *Superset*: http://localhost:8088 (admin/admin)

== ğŸ”„ Data Flow

1. The data generator produces sample user activity and sensor data to Kafka topics
2. Flink jobs consume data from Kafka, process it, and write to Iceberg tables
3. Trino provides SQL querying capabilities over the Iceberg tables
4. Superset connects to Trino for data visualization and dashboarding

== ğŸ“ Project Structure

[source]
----
.
â”œâ”€â”€ data-generator/              # Data generator application (uses Confluent Kafka library)
â”‚   â”œâ”€â”€ Dockerfile               # Docker image definition
â”‚   â”œâ”€â”€ requirements.txt         # Python dependencies
â”‚   â””â”€â”€ data_generator.py        # Data generator script (configurable via property files)
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ cloud/                   # Cloud environment configuration
â”‚   â”‚   â”œâ”€â”€ kafka.properties     # Confluent Cloud Kafka configuration
â”‚   â”‚   â””â”€â”€ topics.properties    # Kafka topic names for cloud
â”‚   â””â”€â”€ local/                   # Local environment configuration
â”‚       â”œâ”€â”€ kafka.properties     # Local Kafka configuration
â”‚       â””â”€â”€ topics.properties    # Kafka topic names for local
â”œâ”€â”€ docker-compose.yml           # Docker Compose configuration
â”œâ”€â”€ flink-jobs/                  # Flink processing jobs
â”‚   â”œâ”€â”€ build.gradle.kts         # Gradle build configuration
â”‚   â”œâ”€â”€ settings.gradle.kts      # Gradle settings
â”‚   â”œâ”€â”€ create_tables.sql        # SQL script to create Iceberg tables
â”‚   â””â”€â”€ src/                     # Source code
â”‚       â””â”€â”€ main/
â”‚           â””â”€â”€ java/
â”‚               â””â”€â”€ com/
â”‚                   â””â”€â”€ example/
â”‚                       â”œâ”€â”€ UserActivityProcessor.java  # User activity processor
â”‚                       â””â”€â”€ SensorDataProcessor.java    # Sensor data processor
â”œâ”€â”€ trino/                       # Trino configuration
â”‚   â””â”€â”€ etc/                     # Trino configuration files
â”‚       â”œâ”€â”€ config.properties    # Server configuration
â”‚       â”œâ”€â”€ jvm.config           # JVM configuration
â”‚       â”œâ”€â”€ node.properties      # Node configuration
â”‚       â”œâ”€â”€ log.properties       # Logging configuration
â”‚       â””â”€â”€ catalog/             # Catalog configurations
â”‚           â”œâ”€â”€ iceberg.properties  # Iceberg catalog
â”‚           â””â”€â”€ memory.properties   # Memory catalog
â”œâ”€â”€ Makefile                     # Project automation
â”œâ”€â”€ requirements.adoc            # Project requirements
â””â”€â”€ README.adoc                  # Project documentation
----

== ğŸ”„ Dependency Management

This project uses https://docs.renovatebot.com/[Renovate] for automated dependency management. Renovate will automatically create pull requests to update dependencies in the project.

=== Key Features

* *Docker Updates*: Automatically updates Docker images with minor and patch versions
* *Package Grouping*: Related packages (Flink, Iceberg, Jackson) are grouped together
* *Dependency Types*:
** Java/Kotlin dependencies via Gradle
** Python dependencies in requirements.txt files
** Docker images in docker-compose.yml
* *Schedule*: Updates are scheduled to run on weekends

The configuration is stored in both `renovate.json` and `.github/renovate.json` files.

== ğŸ”§ Troubleshooting

* *Services not starting*: Check Docker logs with `docker-compose logs <service-name>`
* *Kafka topics not created*: Ensure Kafka is running properly
* *Flink jobs failing*: Check Flink logs in the Flink Dashboard
* *Trino queries failing*: Verify Iceberg REST Catalog and Minio are accessible
* *Superset connection issues*: Ensure Trino is running and accessible

== ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.
