= Data Pipeline Project Requirements Document (PRD)
:toc:
:icons: font

== 1. Project Overview

=== 1.1 Purpose
This project aims to create a comprehensive, end-to-end data pipeline using modern open-source technologies.
The pipeline will demonstrate the integration of streaming data processing, data storage, querying, and visualization capabilities in a portable, containerized environment.

=== 1.2 Components
The data pipeline will include the following components:

* 🚀 *Apache Kafka*: For real-time data ingestion and message queuing
* 🌊 *Apache Flink*: For stream processing and transformation
* ❄️ *Apache Iceberg*: For table format and data lake management
* 🔍 *Trino*: For distributed SQL query engine capabilities
* 📊 *Apache Superset*: For data visualization and dashboarding

=== 1.3 Goals

* Create a fully functional data pipeline that demonstrates real-time data processing
* Ensure all components are containerized using Docker Compose for portability
* Provide clear documentation for setup, configuration, and usage
* Demonstrate best practices for data engineering and architecture

== 2. System Architecture

=== 2.1 High-Level Architecture
[source]
----
┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│             │    │             │    │             │    │             │    │             │
│  Data       │ -> │  Apache     │ -> │  Apache     │ -> │  Apache     │ -> │  Trino      │ -> ┌─────────────┐
│  Source     │    │  Kafka      │    │  Flink      │    │  Iceberg    │    │  Query      │    │             │
│             │    │             │    │             │    │             │    │  Engine     │    │  Apache     │
└─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘    │  Superset   │
                                                                                               │             │
                                                                                               └─────────────┘
----

=== 2.2 Component Interactions

1. *Data Source → Kafka*: Raw data is ingested into Kafka topics
2. *Kafka → Flink*: Flink consumes data from Kafka for processing
3. *Flink → Iceberg*: Processed data is written to Iceberg tables
4. *Iceberg → Trino*: Trino queries data stored in Iceberg format
