# Flink-Trino-Superset Data Pipeline

This project demonstrates an end-to-end data pipeline using Apache Kafka, Apache Flink, Apache Iceberg, Trino, and Apache Superset. The pipeline ingests streaming data, processes it in real-time, stores it in a data lake format, and provides SQL querying and visualization capabilities.

## Architecture

```
┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│             │    │             │    │             │    │             │    │             │
│  Data       │ -> │  Apache     │ -> │  Apache     │ -> │  Apache     │ -> │  Trino      │ -> ┌─────────────┐
│  Generator  │    │  Kafka      │    │  Flink      │    │  Iceberg    │    │  Query      │    │             │
│             │    │             │    │             │    │             │    │  Engine     │    │  Apache     │
└─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘    │  Superset   │
                                                                                               │             │
                                                                                               └─────────────┘
```

## Components

- **Apache Kafka**: Message broker for data ingestion
- **Apache Flink**: Stream processing framework
- **Apache Iceberg**: Table format for data lake storage
- **Trino**: Distributed SQL query engine
- **Apache Superset**: Data visualization and dashboarding

## Prerequisites

- Docker and Docker Compose
- Java 11+ (for building Flink jobs)
- Maven (for building Flink jobs)

## Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/flink-trino-superset-pipeline.git
cd flink-trino-superset-pipeline
```

### 2. Start the Services

```bash
docker-compose up -d
```

This will start all the required services:
- Zookeeper (port 2181)
- Kafka (port 9092)
- Kafka UI (port 8080)
- Minio (ports 9000, 9001)
- Flink JobManager (port 8081)
- Flink TaskManager
- MySQL (port 3306)
- Hive Metastore (port 9083)
- Trino (port 8082)
- Superset (port 8088)
- Data Generator

### 3. Create Iceberg Tables

Connect to the Trino CLI:

```bash
docker exec -it trino-coordinator trino --server localhost:8080 --catalog iceberg
```

Run the SQL script to create tables:

```sql
source /opt/flink/usrlib/create_tables.sql
```

Alternatively, you can run the SQL script directly:

```bash
docker exec -it trino-coordinator trino --server localhost:8080 --catalog iceberg -f /opt/flink/usrlib/create_tables.sql
```

### 4. Submit Flink Jobs

Build the Flink jobs:

```bash
cd flink-jobs
mvn clean package
```

Submit the jobs to Flink:

```bash
docker cp target/flink-iceberg-pipeline-1.0-SNAPSHOT.jar flink-jobmanager:/opt/flink/usrlib/
docker exec -it flink-jobmanager flink run -c com.example.UserActivityProcessor /opt/flink/usrlib/flink-iceberg-pipeline-1.0-SNAPSHOT.jar
docker exec -it flink-jobmanager flink run -c com.example.SensorDataProcessor /opt/flink/usrlib/flink-iceberg-pipeline-1.0-SNAPSHOT.jar
```

### 5. Set Up Superset

Access Superset at http://localhost:8088 and log in with:
- Username: admin
- Password: admin

Configure a connection to Trino:
1. Go to Data -> Databases -> + Database
2. Select "Trino" as the database type
3. Set the SQLAlchemy URI to: `trino://admin@trino-coordinator:8080/iceberg`
4. Test the connection and save

Create datasets and dashboards:
