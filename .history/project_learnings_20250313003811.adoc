= Flink-Trino-Superset Pipeline: Project Learnings
:toc:
:icons: font
:source-highlighter: highlight.js
:imagesdir: images

== üìã Project Overview

This document summarizes the key learnings and issues addressed in the Flink-Trino-Superset data pipeline project.

== üèóÔ∏è Architecture

[source,mermaid]
----
flowchart LR
    DG[Data Generator] --> K[Apache Kafka]
    K --> F[Apache Flink]
    F --> I[Apache Iceberg]
    I --> T[Trino]
    T --> S[Apache Superset]
----

The pipeline consists of the following components:

* *Data Generator*: Python application that produces synthetic user activity and sensor data
* *Apache Kafka*: Message broker for data ingestion (running in KRaft mode without Zookeeper)
* *Apache Flink*: Stream processing framework for real-time data processing
* *Apache Iceberg*: Table format for data lake storage
* *Trino*: Distributed SQL query engine
* *Apache Superset*: Data visualization and dashboarding

== üîÑ Data Flow

1. The *Data Generator* produces two types of sample data:
   * User activity data (page views, clicks, searches, purchases, etc.)
   * Sensor data (temperature, humidity, pressure, light, motion)

2. Data is sent to *Kafka* topics:
   * `user-activity` topic for user events
   * `sensor-data` topic for sensor readings

3. *Flink* jobs consume data from Kafka:
   * `UserActivityProcessor` processes user activity events
   * `SensorDataProcessor` processes sensor data

4. Processed data is written to *Iceberg* tables:
   * `iceberg.warehouse.user_activity`
   * `iceberg.warehouse.sensor_data`

5. *Trino* provides SQL querying capabilities over the Iceberg tables
   * Pre-defined views for analytics: `hourly_user_activity` and `sensor_stats`

6. *Superset* connects to Trino for data visualization and dashboarding

== üîç Key Components Analysis

=== Data Generator

The data generator is a Python application that:

* Uses the Faker library to generate realistic data
* Produces two types of events: user activity and sensor data
* Sends data to Kafka topics with configurable rates
* Simulates real-world patterns with randomized data

=== Flink Jobs

Two Flink streaming jobs process data from Kafka:

==== UserActivityProcessor

* Consumes from the `user-activity` Kafka topic
* Processes user events (page views, clicks, searches, purchases)
* Extracts fields like user_id, event_type, timestamp, session_id
* Currently writes to a file sink (CSV format)

==== SensorDataProcessor

* Consumes from the `sensor-data` Kafka topic
* Processes sensor readings (temperature, humidity, pressure, etc.)
* Extracts fields like sensor_id, sensor_type, timestamp, location
* Currently writes to a file sink (CSV format)

=== Iceberg Tables

Two main tables are defined in the Iceberg catalog:

==== user_activity

[source,sql]
----
CREATE TABLE IF NOT EXISTS iceberg.warehouse.user_activity (
    user_id VARCHAR,
    event_type VARCHAR,
    event_time TIMESTAMP,
    session_id VARCHAR,
    ip_address VARCHAR,
    user_agent VARCHAR,
    page_url VARCHAR,
    total_amount DOUBLE
)
WITH (
    format = 'PARQUET',
    partitioning = ARRAY['day(event_time)']
);
----

==== sensor_data

[source,sql]
----
CREATE TABLE IF NOT EXISTS iceberg.warehouse.sensor_data (
    sensor_id VARCHAR,
    sensor_type VARCHAR,
    event_time TIMESTAMP,
    latitude DOUBLE,
    longitude DOUBLE,
    facility VARCHAR,
    sensor_value DOUBLE
)
WITH (
    format = 'PARQUET',
    partitioning = ARRAY['day(event_time)', 'sensor_type']
);
----

=== Analytics Views

Two pre-defined views for analytics:

==== hourly_user_activity

Aggregates user activity by hour and event type.

==== sensor_stats

Provides statistics on sensor readings by hour, type, and facility.

== üö® Identified Issues and Improvements

=== 1. Flink Job Configuration Issues

*Issue*: The Flink jobs are currently writing to file sinks instead of Iceberg tables.

*Solution*: Update the Flink jobs to use the Flink Iceberg connector to write directly to Iceberg tables. This would involve:

1. Adding Iceberg dependencies to the build.gradle.kts file
2. Configuring Iceberg catalog in the Flink jobs
3. Using the Iceberg sink instead of the file sink

=== 2. Java Version Mismatch

*Issue*: The project guidelines specify Java 21, but the build.gradle.kts file is configured for Java 17.

*Solution*: Update the build.gradle.kts file to use Java 21:

[source,kotlin]
----
java {
    sourceCompatibility = JavaVersion.VERSION_21
    targetCompatibility = JavaVersion.VERSION_21
}

tasks.withType<KotlinCompile> {
    kotlinOptions.jvmTarget = "21"
}
----

=== 3. Kafka Configuration

*Issue*: The README mentions Zookeeper, but the docker-compose.yml is correctly configured for Kafka in KRaft mode without Zookeeper.

*Solution*: Update the README to reflect the current Kafka configuration using KRaft mode.

=== 4. Timestamp Handling

*Issue*: The Flink jobs are not properly parsing the timestamp strings into Flink's timestamp types.

*Solution*: Add proper timestamp parsing in the Flink jobs:

[source,java]
----
// Parse timestamp
String timestampStr = jsonNode.get("timestamp").asText();
Timestamp timestamp = Timestamp.valueOf(LocalDateTime.parse(timestampStr, DateTimeFormatter.ISO_DATE_TIME));
----

=== 5. Error Handling

*Issue*: The Flink jobs lack robust error handling for malformed data.

*Solution*: Add error handling and dead-letter queues for processing failures:

[source,java]
----
// Add a side output for invalid records
final OutputTag<String> invalidRecordsTag = new OutputTag<String>("invalid-records"){};

// Process with error handling
SingleOutputStreamOperator<Tuple7<...>> processedStream = 
    kafkaStream.process(new ProcessFunction<String, Tuple7<...>>() {
        @Override
        public void processElement(String value, Context ctx, Collector<Tuple7<...>> out) {
            try {
                // Process record
                // ...
                out.collect(result);
            } catch (Exception e) {
                // Send to invalid records side output
                ctx.output(invalidRecordsTag, "Error processing record: " + value + ", Error: " + e.getMessage());
            }
        }
    });

// Get the invalid records stream
DataStream<String> invalidRecords = processedStream.getSideOutput(invalidRecordsTag);
// Write invalid records to a separate sink for later analysis
invalidRecords.sinkTo(...);
----

=== 6. Makefile Improvements

*Issue*: The Makefile uses sleep in some places instead of wait/retry logic as specified in the guidelines.

*Solution*: Replace sleep commands with proper wait/retry logic as implemented in the `wait-for-service` function.

=== 7. Documentation Format

*Issue*: Some documentation is in Markdown format instead of AsciiDoc as specified in the guidelines.

*Solution*: Convert README.md to README.adoc using the AsciiDoc format.

== üõ†Ô∏è Best Practices

=== Flink Development

1. *Use Flink Table API or SQL*: According to the guidelines, prefer using Flink Table API or Flink SQL instead of the DataStream API for simpler processing logic.

2. *Proper Checkpoint Configuration*: Configure checkpointing for fault tolerance:
   ```java
   env.enableCheckpointing(60000); // Checkpoint every minute
   env.getCheckpointConfig().setCheckpointTimeout(30000);
   env.getCheckpointConfig().setMinPauseBetweenCheckpoints(30000);
   ```

3. *Watermark Strategy*: Implement proper watermark strategies for event time processing:
   ```java
   WatermarkStrategy<JsonNode> watermarkStrategy = 
       WatermarkStrategy
           .<JsonNode>forBoundedOutOfOrderness(Duration.ofSeconds(5))
           .withTimestampAssigner((event, timestamp) -> {
               // Extract timestamp from event
               return extractTimestamp(event);
           });
   ```

=== Docker and Infrastructure

1. *Use OrbStack*: The guidelines recommend using OrbStack instead of Docker Desktop for managing containers.

2. *Docker Compose*: Don't add version attribute to docker-compose.yaml file.

3. *Kafka Configuration*: Use Apache Kafka 3.9.0 in KRaft mode without Zookeeper.

=== Build and Deployment

1. *Use Gradle with Kotlin*: The project uses Gradle with Kotlin DSL for build scripts.

2. *Use SDKMAN*: Recommended for managing Java SDKs.

3. *GitHub Actions*: Create GitHub Actions based on commands in the Makefile.

== üìä Monitoring and Observability

The pipeline includes several monitoring points:

1. *Kafka UI*: Available at http://localhost:8080 for monitoring Kafka topics and consumers.

2. *Flink Dashboard*: Available at http://localhost:8081 for monitoring Flink jobs and performance.

3. *Trino UI*: Available at http://localhost:8082 for monitoring query execution.

4. *Minio Console*: Available at http://localhost:9001 for monitoring S3 storage.

5. *Superset*: Available at http://localhost:8088 for data visualization and dashboards.

== üöÄ Getting Started

1. Clone the repository
2. Run `make up` to start all services
3. Run `make create-tables` to create Iceberg tables
4. Run `make deploy-flink-jobs` to deploy Flink jobs
5. Access Superset at http://localhost:8088 (admin/admin) to create dashboards

For a complete demo, run `make demo` which will:
1. Build all components
2. Start all services
3. Create tables
4. Deploy Flink jobs
5. Run example queries
6. Set up Superset

== üîÆ Future Improvements

1. *Add Schema Registry*: Implement Confluent Schema Registry for schema evolution.

2. *Implement CDC Pipeline*: Add Change Data Capture from databases.

3. *Add Unit and Integration Tests*: Improve test coverage.

4. *Implement CI/CD Pipeline*: Automate testing and deployment.

5. *Add Monitoring and Alerting*: Implement Prometheus and Grafana for monitoring.

6. *Implement Data Quality Checks*: Add data quality validation in the pipeline.

7. *Optimize Performance*: Fine-tune Flink and Trino for better performance.
